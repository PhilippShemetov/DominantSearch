{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-07 14:56:46.436160: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-12-07 14:56:47.224623: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2022-12-07 14:56:49.013585: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/silver22/.local/lib/python3.10/site-packages/cv2/../../lib64:/usr/local/cuda-11.7/lib64\n",
      "2022-12-07 14:56:49.013819: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/silver22/.local/lib/python3.10/site-packages/cv2/../../lib64:/usr/local/cuda-11.7/lib64\n",
      "2022-12-07 14:56:49.013839: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "import cv2\n",
    "import torch\n",
    "import comet_ml\n",
    "import os\n",
    "import glob\n",
    "import re\n",
    "import tensorflow as tf\n",
    "from functools import partial\n",
    "from retinaface import RetinaFace\n",
    "import torch\n",
    "from natsort import natsorted\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import random_split\n",
    "from torchvision.datasets import MNIST\n",
    "from torchvision import transforms\n",
    "\n",
    "import torchmetrics\n",
    "import pytorch_lightning as pl\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Инфо по версии CUDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n",
      "CUDA:  11.7\n",
      "IS CUDA AVAILABLE:  True\n"
     ]
    }
   ],
   "source": [
    "def get_cuda_info():\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(device)\n",
    "    print(\"CUDA: \",torch.version.cuda)\n",
    "    print(\"IS CUDA AVAILABLE: \",torch.cuda.is_available())\n",
    "    \n",
    "get_cuda_info()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Формируем датасет самых и менее доминантных персон"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       name  start (sec)  end (sec)  A11  A12  A13  A14  A21  A22  A23  A24  \\\n",
      "0   IS1000a            0        300    4    1    3    2    4    1    3    2   \n",
      "1   IS1000a          300        600    3    1    4    2    3    1    4    2   \n",
      "2   IS1000a          600        900    1    4    2    3    1    2    4    3   \n",
      "4   IS1000a         1200       1500    4    1    3    2    3    1    4    2   \n",
      "6   IS1001a          300        600    1    2    4    3    1    2    4    3   \n",
      "7   IS1001a          600        900    1    2    3    4    1    2    3    4   \n",
      "9   IS1001b          300        600    1    3    4    2    1    2    4    3   \n",
      "12  IS1001b         1200       1500    1    2    3    4    1    3    2    4   \n",
      "13  IS1001b         1500       1800    1    3    2    4    1    2    3    4   \n",
      "14  IS1001b         1800       2100    1    2    3    4    1    2    3    4   \n",
      "18  IS1001c          900       1200    1    2    3    4    1    2    3    4   \n",
      "19  IS1001c         1200       1445    1    3    2    4    1    3    2    4   \n",
      "24  IS1003b         1200       1500    1    3    4    2    1    3    4    2   \n",
      "25  IS1003b         1500       1646    1    3    2    4    1    3    4    2   \n",
      "27  IS1003d          300        600    1    3    4    2    1    4    3    2   \n",
      "28  IS1003d          600        900    1    2    3    4    1    3    4    2   \n",
      "29  IS1003d          900       1200    1    2    4    3    1    4    3    2   \n",
      "30  IS1003d         1200       1500    1    3    4    2    1    3    4    2   \n",
      "31  IS1003d         1800       2100    1    4    2    3    1    2    3    4   \n",
      "32  IS1006b            0        300    2    4    3    1    2    4    3    1   \n",
      "34  IS1006b          600        900    2    1    4    3    2    1    4    3   \n",
      "36  IS1006b         1200       1500    2    4    3    1    2    4    3    1   \n",
      "37  IS1006b         1500       1800    2    4    1    3    2    3    1    4   \n",
      "40  IS1008a          300        600    1    4    2    3    1    4    2    3   \n",
      "41  IS1008a          600        900    1    2    4    3    1    2    4    3   \n",
      "42  IS1008b            0        300    1    2    3    4    1    2    3    4   \n",
      "43  IS1008b          300        600    2    1    3    4    2    1    3    4   \n",
      "46  IS1008b         1200       1500    1    4    2    3    1    4    2    3   \n",
      "47  IS1008b         1500       1768    2    1    3    4    3    1    2    4   \n",
      "48  IS1008c            0        300    2    4    1    3    2    3    1    4   \n",
      "49  IS1008c          300        600    4    2    1    3    2    4    1    3   \n",
      "52  IS1008c         1200       1500    1    4    2    3    1    4    2    3   \n",
      "55  IS1008d          600        900    3    2    4    1    4    2    3    1   \n",
      "57  IS1008d         1200       1480    2    1    3    4    2    1    4    3   \n",
      "\n",
      "    A31  A32  A33  A34  FMD  FLD  \n",
      "0     4    1    3    2    1    1  \n",
      "1     3    1    4    2    1    1  \n",
      "2     1    4    3    2    1    0  \n",
      "4     3    1    4    2    1    0  \n",
      "6     1    2    4    3    1    1  \n",
      "7     1    2    3    4    1    1  \n",
      "9     1    2    4    3    1    1  \n",
      "12    1    2    3    4    1    1  \n",
      "13    1    2    3    4    1    1  \n",
      "14    1    2    3    4    1    1  \n",
      "18    1    2    3    4    1    1  \n",
      "19    1    3    2    4    1    1  \n",
      "24    1    2    3    4    1    0  \n",
      "25    1    4    3    2    1    0  \n",
      "27    1    4    3    2    1    0  \n",
      "28    1    3    4    2    1    0  \n",
      "29    1    4    3    2    1    0  \n",
      "30    1    2    4    3    1    1  \n",
      "31    1    4    3    2    1    0  \n",
      "32    3    4    2    1    1    1  \n",
      "34    3    1    4    2    1    1  \n",
      "36    2    3    4    1    1    0  \n",
      "37    3    4    1    2    1    0  \n",
      "40    1    4    2    3    1    1  \n",
      "41    1    2    4    3    1    1  \n",
      "42    1    2    3    4    1    1  \n",
      "43    2    1    3    4    1    1  \n",
      "46    1    4    2    3    1    1  \n",
      "47    3    1    2    4    1    1  \n",
      "48    2    3    1    4    1    0  \n",
      "49    2    3    1    4    1    0  \n",
      "52    1    3    2    4    1    0  \n",
      "55    3    2    4    1    1    0  \n",
      "57    2    1    3    4    1    0  \n"
     ]
    }
   ],
   "source": [
    "def get_name_of_folders_with_video():\n",
    "    set_of_video = ['IS1000a','IS1001a','IS1001b','IS1001c','IS1003b','IS1003d','IS1006b','IS1008a',\n",
    "                'IS1008b','IS1008c','IS1008d']\n",
    "    return set_of_video\n",
    "\n",
    "def get_csv_data(type_data=None):\n",
    "    annot_data = pd.read_csv('dome_annotations_M1.csv')\n",
    "    dome_dataset = pd.read_csv('dome_dataset_M1.csv')\n",
    "    annot_data = annot_data.join(dome_dataset['FMD'])\n",
    "    annot_data = annot_data.join(dome_dataset['FLD'])\n",
    "    if type_data == \"FMD\":\n",
    "        annot_data = annot_data[annot_data['FMD'] == 1]\n",
    "    elif type_data == \"FLD\":\n",
    "        annot_data = annot_data[annot_data['FLD'] == 1]\n",
    "    \n",
    "    return annot_data\n",
    "        \n",
    "        \n",
    "print(get_csv_data(\"FMD\"))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Получаем кадры из видео "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_images_from_video(image_output_path, video_input_path, frame_skip = 5, \n",
    "                             duration = 60, type_data = \"FMD\"):\n",
    "    set_of_video = get_name_of_folders_with_video()\n",
    "    csv_data = get_csv_data(type_data)\n",
    "    \n",
    "    for name_video_folder in set_of_video:\n",
    "        part_data = csv_data.loc[csv_data['name'] == name_video_folder]\n",
    "        start_time = part_data['start (sec)'].values * 25\n",
    "        end_time = part_data['end (sec)'].values * 25\n",
    "        video_input_path = f\"{video_input_path}/{name_video_folder}\"\n",
    "        len_of_data = len(part_data)\n",
    "        \n",
    "        for number_person in range(0,4):\n",
    "            vidcap = cv2.VideoCapture(f'amicorpus/{name_video_folder}/video/{name_video_folder}.Closeup{number_person+1}.avi')\n",
    "            success,image = vidcap.read()\n",
    "            count = 0\n",
    "            index = 0\n",
    "            step_unit_time = 0\n",
    "            duration_frames = duration * 25\n",
    "            bin_count = 0\n",
    "            while success:\n",
    "                temp_data = part_data.iloc[index].values[3:7].tolist()\n",
    "                dom_person = 0\n",
    "                if type_data == \"FMD\":\n",
    "                    dom_person = temp_data.index(1)\n",
    "                elif type_data == \"FLD\":\n",
    "                    dom_person = temp_data.index(4)\n",
    "                if count % frame_skip == 0 and count >= start_time[index] and count <= end_time[index]:\n",
    "                    if count % duration_frames == 0:\n",
    "                        step_unit_time = int(count / duration_frames)\n",
    "                        dir_path = f\"{image_output_path}/{name_video_folder}/{step_unit_time}unit_time_domperson{dom_person}\"\n",
    "                        if not os.path.exists(dir_path):\n",
    "                            os.makedirs(dir_path)\n",
    "                    dir_path = f\"{image_output_path}/{name_video_folder}/{step_unit_time}unit_time_domperson{dom_person}/person{number_person}\"\n",
    "                    if not os.path.exists(dir_path):\n",
    "                        bin_count = 0\n",
    "                        os.makedirs(dir_path) \n",
    "                    bin_count_txt = f'{bin_count:0012b}'\n",
    "                    cv2.imwrite(f\"{dir_path}/{bin_count_txt}.jpg\", image)     # save frame as JPEG file\n",
    "                    bin_count += 1\n",
    "                success,image = vidcap.read()\n",
    "                if count >= end_time[index] - 1 and len_of_data-1 > index:\n",
    "                    index += 1\n",
    "                count += 1\n",
    "                        \n",
    "create_images_from_video(image_output_path=\"test_output\",video_input_path=\"amicorpus\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for name_video_folder in set_of_video:\n",
    "#   part_data = annot_data.loc[annot_data['name'] == name_video_folder]\n",
    "#   len_of_data = len(part_data)\n",
    "#   start_time = part_data['start (sec)'].values * 25 #number of frames\n",
    "#   end_time = part_data['end (sec)'].values * 25\n",
    "#   for number_person in range(0,4):\n",
    "#     vidcap = cv2.VideoCapture(f'amicorpus/{name_video_folder}/video/{name_video_folder}.Closeup{number_person+1}.avi')\n",
    "#     success,image = vidcap.read()\n",
    "#     count = 0\n",
    "#     index = 0\n",
    "#     bin_count = 0\n",
    "#     time_frames = 0\n",
    "#     while success:\n",
    "#       temp_data = part_data.iloc[index].values[3:7].tolist()\n",
    "#       dom_persone = temp_data.index(1)\n",
    "#       if count % 5 == 0 and count >= start_time[index] and count <= end_time[index] :\n",
    "#         #print(count)\n",
    "#         if count % 1500 == 0:\n",
    "#           time_frames = int(count / 1500)\n",
    "#           dir1 = os.path.join(\"NewImages/\", f\"{name_video_folder}\", f\"{time_frames}min_domperson{dom_persone}\")\n",
    "#           if not os.path.exists(dir1):\n",
    "#             os.mkdir(dir1)\n",
    "#         dir2 = os.path.join(f\"NewImages/{name_video_folder}\", f\"{time_frames}min_domperson{dom_persone}\", f\"person{number_person}\")\n",
    "#         if not os.path.exists(dir2):\n",
    "#           bin_count = 0\n",
    "#           os.mkdir(dir2)\n",
    "#         bin_count_txt = f'{bin_count:0012b}'\n",
    "#         cv2.imwrite(f\"NewImages/{name_video_folder}/{time_frames}min_domperson{dom_persone}/person{number_person}/{bin_count_txt}.jpg\", image)\n",
    "#         bin_count += 1\n",
    "#       success,image = vidcap.read()\n",
    "#       #print(count,\" \",dom_persone)\n",
    "#       if count >= end_time[index] - 1 and len_of_data-1 > index:\n",
    "#         index += 1\n",
    "#       count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pixel(img, center, x, y):\n",
    "\tnew_value = 0\n",
    "\ttry:\n",
    "\t\tif img[x][y] >= center:\n",
    "\t\t\tnew_value = 1\n",
    "\texcept:\n",
    "\t\tpass\n",
    "\n",
    "\treturn new_value\n",
    "\n",
    "# Function for calculating LBP\n",
    "def lbp_calculated_pixel(img, x, y):\n",
    "\tcenter = img[x][y]\n",
    "\tval_ar = []\n",
    "\n",
    "\tval_ar.append(get_pixel(img, center, x-1, y-1))\n",
    "\t\n",
    "\tval_ar.append(get_pixel(img, center, x-1, y))\n",
    "\t\n",
    "\tval_ar.append(get_pixel(img, center, x-1, y + 1))\n",
    "\n",
    "\tval_ar.append(get_pixel(img, center, x, y + 1))\n",
    "\t\n",
    "\tval_ar.append(get_pixel(img, center, x + 1, y + 1))\n",
    "\t\n",
    "\tval_ar.append(get_pixel(img, center, x + 1, y))\n",
    "\t\n",
    "\tval_ar.append(get_pixel(img, center, x + 1, y-1))\n",
    "\t\n",
    "\tval_ar.append(get_pixel(img, center, x, y-1))\n",
    "\t\n",
    "\tpower_val = [1, 2, 4, 8, 16, 32, 64, 128]\n",
    "\n",
    "\tval = 0\n",
    "\t\n",
    "\tfor i in range(len(val_ar)):\n",
    "\t\tval += val_ar[i] * power_val[i]\n",
    "\t\t\n",
    "\treturn val\n",
    "\n",
    "def LBP(img, align = False, shape = (128, 128)):\n",
    "    height, width = shape\n",
    "    img_lbp = np.zeros((height, width),\n",
    "\t\t\t\tnp.uint8)\n",
    "    for i in range(0, height):\n",
    "        for j in range(0, width):\n",
    "            img_lbp[i, j] = lbp_calculated_pixel(img, i, j)\n",
    "    return img_lbp\n",
    "\n",
    "def get_features_LBP(img, align = False, shape = (128, 128)):\n",
    "\tif img.sum() == 0:\n",
    "\t\treturn np.zeros(256)\n",
    "\telse:\n",
    "\t\tfeature = LBP(img, align, shape)\n",
    "\t\tmean_img = np.mean(feature, axis = 0)\n",
    "\t\tstd_img = np.std(feature, axis = 0)\n",
    "\t\tfeature = np.concatenate((mean_img, std_img))\n",
    "\t\treturn feature\n",
    "\t#print(feature)\n",
    "\t#new_feature = np.array([mean_img, std_img])\n",
    "\t#print(mean_img)\n",
    "# def get_features():\n",
    "#     #s = start_time()\n",
    "#     X_data = []\n",
    "#     y_data = []\n",
    "#     for name_video_folder in set_of_video:\n",
    "#         for idx, dom_name in enumerate(['NonDominant','MostDominant']):\n",
    "#             for number_person in range(1,5):\n",
    "#                 count = 1\n",
    "#                 temp_arr = np.zeros([128,1],dtype=int)\n",
    "#                 list_of_files = glob.glob(f'Face_Dominant_32x32/{dom_name}/{name_video_folder}/person{number_person}/*')\n",
    "#                 for image_path in tqdm(list_of_files):\n",
    "#                     gray = cv2.imread(image_path,cv2.IMREAD_GRAYSCALE)\n",
    "#                     if count % 300 == 0:\n",
    "#                         X_data.append(temp_arr)\n",
    "#                         y_data.append(idx)\n",
    "#                         temp_arr = np.zeros([128,1],dtype=int)\n",
    "#                     else:\n",
    "#                         gray_img = LBP(gray)\n",
    "#                         print(np.shape(gray_img))\n",
    "#                         mean_img = gray_img.mean(0)\n",
    "#                         std_img = gray_img.std(0)\n",
    "#                         print(np.shape(mean_img[0]))\n",
    "                        \n",
    "#                         temp_arr = np.append(temp_arr,LBP(gray),1)\n",
    "#                     count += 1\n",
    "#                     # print(np.shape(X_data))\n",
    "                    \n",
    "\n",
    "#     # for image_path in list_of_files:\n",
    "#     #     if count % 1000 == 999:\n",
    "#     #         print(file_path,\" - \", count)\n",
    "#     #     gray = cv2.imread(image_path,cv2.IMREAD_GRAYSCALE)\n",
    "#     #     arr_img = LBP(gray)\n",
    "#     #     X_data.append(arr_img)\n",
    "#     #     y_data.append(file_path)\n",
    "#     #     count += 1\n",
    "\n",
    "#     return X_data, y_data\n",
    "\n",
    "# X,y = get_features()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def face_detector(image_input_path, image_output_path, resize=False, shape=(128,128), face_extractor=None):\n",
    "    face_extractor = RetinaFace \n",
    "    # if face_extractor == \"RetinaFace\":\n",
    "    #     face_extractor = RetinaFace\n",
    "    # elif face_extractor == \"MTCNN\":\n",
    "    #     face_extractor = MTCNN\n",
    "    # elif face_extractor == \"FaceNet\":\n",
    "    #     face_extractor = FaceNet\n",
    "    # elif face_extractor == \"LBP\":\n",
    "    #     face_extractor = partial(LBP, shape=shape)\n",
    "        \n",
    "    \n",
    "    set_of_video = get_name_of_folders_with_video()\n",
    "    for name_video_folder in set_of_video:\n",
    "        list_of_dirs = os.listdir(f\"{image_input_path}/{name_video_folder}\")\n",
    "        for dir_name in list_of_dirs:\n",
    "            for number_person in range(0,4):\n",
    "                dir_path = f\"{image_output_path}/{name_video_folder}/{dir_name}/person{number_person}\"\n",
    "                if not os.path.exists(dir_path):\n",
    "                    os.makedirs(dir_path)\n",
    "                list_of_path_images = glob.glob(f\"{image_input_path}/{name_video_folder}/{dir_name}/person{number_person}/*\")\n",
    "                \n",
    "                for path_image in tqdm(list_of_path_images):\n",
    "                    image_name = path_image.split(\"/\")[-1]\n",
    "                    faces = face_extractor.extract_faces(img_path = path_image, align = False)\n",
    "                    if faces:\n",
    "                        for idx, face in enumerate(faces):\n",
    "                            if resize:\n",
    "                                face = cv2.resize(face, shape)\n",
    "                            face = cv2.cvtColor(face, cv2.COLOR_BGR2GRAY)\n",
    "                            cv2.imwrite(f\"{dir_path}/{idx}id_{image_name}\", face)\n",
    "                    else:\n",
    "                        black_image = np.zeros(shape, dtype=np.uint8)\n",
    "                        cv2.imwrite(f\"{dir_path}/0id_{image_name}\", black_image)\n",
    "                \n",
    "        \n",
    "\n",
    "face_detector(image_input_path=\"NewImages\", image_output_path=\"test_output_2\", resize=True, face_extractor=None)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for name_video_folder in set_of_video:\n",
    "#     list_of_files = glob.glob(f'NewImages/{name_video_folder}/*')\n",
    "#     for files in list_of_files:\n",
    "#         dir_1 = f'Face_Dominant128x128/{files[10:]}'\n",
    "#         if not os.path.exists(dir_1):\n",
    "#             os.mkdir(dir_1)\n",
    "#         for number_person in range(0,4):\n",
    "#             dir_2 = f'Face_Dominant128x128/{files[10:]}/person{number_person}'\n",
    "#             if not os.path.exists(dir_2):\n",
    "#                 os.mkdir(dir_2)\n",
    "            \n",
    "#             list_of_images = glob.glob(f'{files}/person{number_person}/*')\n",
    "#             for image_file in tqdm(list_of_images):\n",
    "#                 faces = RetinaFace.extract_faces(img_path = image_file, align = False)\n",
    "#                 if faces:\n",
    "#                     img_res = cv2.resize(faces[0],(128,128))\n",
    "#                     cv2.imwrite(f'Face_Dominant128x128/{image_file[10:]}',cv2.cvtColor(img_res, cv2.COLOR_BGR2GRAY))\n",
    "#                 else:\n",
    "#                     black_image = np.zeros((128,128,1), np.uint8)\n",
    "#                     cv2.imwrite(f'Face_Dominant128x128/{image_file[10:]}',black_image)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_extraction(image_input_path, feature_extractor=None, save_csv=False, csv_path=None):\n",
    "    feature_extractor = get_features_LBP\n",
    "    dict_template = {\"person0_features\":[],\"person1_features\":[],\"person2_features\":[],\"person3_features\":[],\"id_video\":[],\"labels\":[]}\n",
    "    csv_result = pd.DataFrame(dict_template)\n",
    "    \n",
    "    set_of_video = get_name_of_folders_with_video()\n",
    "    for name_video_folder in set_of_video:\n",
    "        list_of_dirs = os.listdir(f\"{image_input_path}/{name_video_folder}\")\n",
    "        for dir_name in tqdm(list_of_dirs):\n",
    "            dom_person = dir_name[-1]\n",
    "            dict_csv = {\"person0_features\":[],\"person1_features\":[],\"person2_features\":[],\"person3_features\":[],\"id_video\":name_video_folder,\"labels\":dom_person}\n",
    "            for number_person in range(0,4):\n",
    "                person_features = []\n",
    "                list_of_path_images = glob.glob(f\"{image_input_path}/{name_video_folder}/{dir_name}/person{number_person}/*\")\n",
    "                for path_image in list_of_path_images:\n",
    "                    image = cv2.imread(path_image, cv2.IMREAD_GRAYSCALE)\n",
    "                    feature = feature_extractor(image)\n",
    "                    person_features.append(feature)\n",
    "                mean_person_features = np.mean(person_features, axis=0)\n",
    "                dict_csv[f\"person{number_person}_features\"].append(mean_person_features)\n",
    "            csv_temp = pd.DataFrame(dict_csv)\n",
    "            csv_result = pd.concat([csv_result, csv_temp], ignore_index=True)\n",
    "    if save_csv:\n",
    "        csv_result.to_csv(csv_path)\n",
    "        \n",
    "    \n",
    "        \n",
    "\n",
    "feature_extraction(image_input_path=\"Face_Dominant128x128\", save_csv=True, csv_path=\"csv_LBP.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_csv(csv_path):\n",
    "    return pd.read_csv(csv_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeaturesDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, csv_file, transform=None):\n",
    "        self.data = csv_file\n",
    "        self.person0 = self.data[\"person0_features\"].apply(lambda x: np.array(x[1:-1].split(), dtype=np.float32))\n",
    "        self.person1 = self.data[\"person1_features\"].apply(lambda x: np.array(x[1:-1].split(), dtype=np.float32))\n",
    "        self.person2 = self.data[\"person2_features\"].apply(lambda x: np.array(x[1:-1].split(), dtype=np.float32))\n",
    "        self.person3 = self.data[\"person3_features\"].apply(lambda x: np.array(x[1:-1].split(), dtype=np.float32))\n",
    "        self.person0_norm = self.person0.apply(lambda x: x/np.linalg.norm(x)).apply(lambda x: np.nan_to_num(x))\n",
    "        self.person1_norm = self.person1.apply(lambda x: x/np.linalg.norm(x)).apply(lambda x: np.nan_to_num(x))\n",
    "        self.person2_norm = self.person2.apply(lambda x: x/np.linalg.norm(x)).apply(lambda x: np.nan_to_num(x))\n",
    "        self.person3_norm = self.person3.apply(lambda x: x/np.linalg.norm(x)).apply(lambda x: np.nan_to_num(x))\n",
    "        self.transform = transform\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        data = self.data.iloc[idx]\n",
    "        person0 = self.person0[idx]\n",
    "        person1 = self.person1[idx]\n",
    "        person2 = self.person2[idx]\n",
    "        person3 = self.person3[idx]\n",
    "        label = int(data['labels'])\n",
    "    \n",
    "        return person0, person1, person2, person3, label\n",
    "        \n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "\n",
    "# csv_res1 = pd.read_csv(\"csv_LBP.csv\")\n",
    "# csv_res1['person0_features'].apply(lambda x: np.array(x[1:-1].split(), dtype=np.float32))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_val_test_split(csv_file):\n",
    "    train_csv = csv_file[(csv_file['id_video'] != 'IS1003b') & (csv_file['id_video'] != 'IS1001c') & (csv_file['id_video'] != 'IS1008c')]\n",
    "    train_csv = train_csv.reset_index(drop=True)\n",
    "    val_csv = csv_file[(csv_file['id_video'] == 'IS1003b') | (csv_file['id_video'] == 'IS1001c')]\n",
    "    val_csv = val_csv.reset_index(drop=True)\n",
    "    test_csv = csv_file[(csv_file['id_video'] == 'IS1008c')]\n",
    "    test_csv = test_csv.reset_index(drop=True)\n",
    "    return train_csv, val_csv, test_csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "131 15 15\n"
     ]
    }
   ],
   "source": [
    "result_csv = get_csv(\"csv_LBP.csv\")\n",
    "\n",
    "train_csv, val_csv, test_csv = train_val_test_split(result_csv)\n",
    "\n",
    "print(len(train_csv), len(val_csv), len(test_csv))\n",
    "\n",
    "# train_csv = result_csv[(result_csv['id_video'] != 'IS1003b') & (result_csv['id_video'] != 'IS1001c') & (result_csv['id_video'] != 'IS1008c')]\n",
    "# train_csv = train_csv.reset_index(drop=True)\n",
    "# val_csv = result_csv[(result_csv['id_video'] == 'IS1003b') | (result_csv['id_video'] == 'IS1001c')]\n",
    "# val_csv = val_csv.reset_index(drop=True)\n",
    "# test_csv = result_csv[(result_csv['id_video'] == 'IS1008c')]\n",
    "# test_csv = test_csv.reset_index(drop=True)\n",
    "\n",
    "train_loader = DataLoader(FeaturesDataset(train_csv),batch_size=1, num_workers=8)\n",
    "val_loader = DataLoader(FeaturesDataset(val_csv),batch_size=1, num_workers=8)\n",
    "test_loader = DataLoader(FeaturesDataset(test_csv),batch_size=1, num_workers=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "next(iter(train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "name_of_features = ['AU01_r','AU02_r','AU04_r','AU05_r','AU06_r','AU07_r','AU09_r','AU10_r',\n",
    "                    'AU12_r','AU14_r','AU15_r','AU17_r','AU20_r','AU23_r','AU25_r','AU26_r',\n",
    "                    'AU45_r','AU01_c','AU02_c','AU04_c','AU05_c','AU06_c','AU07_c',\n",
    "                    'AU09_c','AU10_c','AU12_c','AU14_c','AU15_c','AU17_c','AU20_c','AU23_c',\n",
    "                    'AU25_c','AU26_c','AU28_c','AU45_c']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_of_video = ['IS1000a','IS1001a','IS1001b','IS1001c','IS1003b','IS1003d','IS1006b','IS1008a',\n",
    "                'IS1008b','IS1008c','IS1008d']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>frame</th>\n",
       "      <th>face_id</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>confidence</th>\n",
       "      <th>success</th>\n",
       "      <th>gaze_0_x</th>\n",
       "      <th>gaze_0_y</th>\n",
       "      <th>gaze_0_z</th>\n",
       "      <th>gaze_1_x</th>\n",
       "      <th>gaze_1_y</th>\n",
       "      <th>...</th>\n",
       "      <th>AU12_c</th>\n",
       "      <th>AU14_c</th>\n",
       "      <th>AU15_c</th>\n",
       "      <th>AU17_c</th>\n",
       "      <th>AU20_c</th>\n",
       "      <th>AU23_c</th>\n",
       "      <th>AU25_c</th>\n",
       "      <th>AU26_c</th>\n",
       "      <th>AU28_c</th>\n",
       "      <th>AU45_c</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.88</td>\n",
       "      <td>1</td>\n",
       "      <td>0.217457</td>\n",
       "      <td>0.033246</td>\n",
       "      <td>-0.975504</td>\n",
       "      <td>-0.182038</td>\n",
       "      <td>0.028498</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.88</td>\n",
       "      <td>1</td>\n",
       "      <td>0.212280</td>\n",
       "      <td>0.039451</td>\n",
       "      <td>-0.976412</td>\n",
       "      <td>-0.191070</td>\n",
       "      <td>0.044660</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.88</td>\n",
       "      <td>1</td>\n",
       "      <td>0.217850</td>\n",
       "      <td>0.070070</td>\n",
       "      <td>-0.973464</td>\n",
       "      <td>-0.195546</td>\n",
       "      <td>0.086792</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.88</td>\n",
       "      <td>1</td>\n",
       "      <td>0.232961</td>\n",
       "      <td>0.072879</td>\n",
       "      <td>-0.969751</td>\n",
       "      <td>-0.184878</td>\n",
       "      <td>0.092050</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.88</td>\n",
       "      <td>1</td>\n",
       "      <td>0.241321</td>\n",
       "      <td>0.095292</td>\n",
       "      <td>-0.965756</td>\n",
       "      <td>-0.176681</td>\n",
       "      <td>0.114356</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>295</th>\n",
       "      <td>296</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.88</td>\n",
       "      <td>1</td>\n",
       "      <td>0.206368</td>\n",
       "      <td>0.247157</td>\n",
       "      <td>-0.946745</td>\n",
       "      <td>-0.061561</td>\n",
       "      <td>0.209678</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>296</th>\n",
       "      <td>297</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.88</td>\n",
       "      <td>1</td>\n",
       "      <td>0.209831</td>\n",
       "      <td>0.298733</td>\n",
       "      <td>-0.930983</td>\n",
       "      <td>-0.021205</td>\n",
       "      <td>0.258392</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>297</th>\n",
       "      <td>298</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.77</td>\n",
       "      <td>1</td>\n",
       "      <td>0.206486</td>\n",
       "      <td>0.305500</td>\n",
       "      <td>-0.929534</td>\n",
       "      <td>-0.012050</td>\n",
       "      <td>0.261493</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>298</th>\n",
       "      <td>299</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.98</td>\n",
       "      <td>1</td>\n",
       "      <td>0.187627</td>\n",
       "      <td>0.245146</td>\n",
       "      <td>-0.951157</td>\n",
       "      <td>-0.038141</td>\n",
       "      <td>0.196185</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>299</th>\n",
       "      <td>300</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.98</td>\n",
       "      <td>1</td>\n",
       "      <td>0.158998</td>\n",
       "      <td>0.247216</td>\n",
       "      <td>-0.955826</td>\n",
       "      <td>-0.063731</td>\n",
       "      <td>0.198913</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>300 rows × 714 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     frame  face_id  timestamp  confidence  success  gaze_0_x  gaze_0_y  \\\n",
       "0        1        0        0.0        0.88        1  0.217457  0.033246   \n",
       "1        2        0        0.0        0.88        1  0.212280  0.039451   \n",
       "2        3        0        0.0        0.88        1  0.217850  0.070070   \n",
       "3        4        0        0.0        0.88        1  0.232961  0.072879   \n",
       "4        5        0        0.0        0.88        1  0.241321  0.095292   \n",
       "..     ...      ...        ...         ...      ...       ...       ...   \n",
       "295    296        0        0.0        0.88        1  0.206368  0.247157   \n",
       "296    297        0        0.0        0.88        1  0.209831  0.298733   \n",
       "297    298        0        0.0        0.77        1  0.206486  0.305500   \n",
       "298    299        0        0.0        0.98        1  0.187627  0.245146   \n",
       "299    300        0        0.0        0.98        1  0.158998  0.247216   \n",
       "\n",
       "     gaze_0_z  gaze_1_x  gaze_1_y  ...  AU12_c  AU14_c  AU15_c  AU17_c  \\\n",
       "0   -0.975504 -0.182038  0.028498  ...     1.0     1.0     0.0     0.0   \n",
       "1   -0.976412 -0.191070  0.044660  ...     1.0     1.0     0.0     0.0   \n",
       "2   -0.973464 -0.195546  0.086792  ...     0.0     1.0     0.0     1.0   \n",
       "3   -0.969751 -0.184878  0.092050  ...     0.0     1.0     0.0     1.0   \n",
       "4   -0.965756 -0.176681  0.114356  ...     0.0     1.0     0.0     1.0   \n",
       "..        ...       ...       ...  ...     ...     ...     ...     ...   \n",
       "295 -0.946745 -0.061561  0.209678  ...     0.0     0.0     0.0     0.0   \n",
       "296 -0.930983 -0.021205  0.258392  ...     0.0     0.0     0.0     0.0   \n",
       "297 -0.929534 -0.012050  0.261493  ...     0.0     0.0     0.0     0.0   \n",
       "298 -0.951157 -0.038141  0.196185  ...     0.0     0.0     1.0     0.0   \n",
       "299 -0.955826 -0.063731  0.198913  ...     0.0     0.0     1.0     0.0   \n",
       "\n",
       "     AU20_c  AU23_c  AU25_c  AU26_c  AU28_c  AU45_c  \n",
       "0       0.0     0.0     1.0     0.0     0.0     1.0  \n",
       "1       1.0     1.0     0.0     0.0     0.0     1.0  \n",
       "2       1.0     0.0     0.0     0.0     0.0     1.0  \n",
       "3       0.0     0.0     0.0     0.0     0.0     1.0  \n",
       "4       0.0     1.0     0.0     0.0     0.0     0.0  \n",
       "..      ...     ...     ...     ...     ...     ...  \n",
       "295     0.0     0.0     0.0     1.0     0.0     0.0  \n",
       "296     0.0     0.0     0.0     1.0     0.0     0.0  \n",
       "297     0.0     0.0     0.0     0.0     0.0     0.0  \n",
       "298     0.0     0.0     0.0     1.0     0.0     0.0  \n",
       "299     0.0     0.0     1.0     1.0     0.0     0.0  \n",
       "\n",
       "[300 rows x 714 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = pd.read_csv('NewOpenFaceFeatures2/IS1000a/7min_domperson1/person1/person1.csv')\n",
    "#test[name_of_features] = 0\n",
    "test.loc[test['confidence'] < 0.8, name_of_features] = 0\n",
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'set_of_video' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [4], line 5\u001b[0m\n\u001b[1;32m      1\u001b[0m column_names \u001b[39m=\u001b[39m [\u001b[39m\"\u001b[39m\u001b[39mperson0_features\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mperson1_features\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mperson2_features\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m      2\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39mperson3_features\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mid_video\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mlabels\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[1;32m      4\u001b[0m result_csv \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mDataFrame(columns \u001b[39m=\u001b[39m column_names)\n\u001b[0;32m----> 5\u001b[0m \u001b[39mfor\u001b[39;00m name_video_folder \u001b[39min\u001b[39;00m set_of_video:\n\u001b[1;32m      6\u001b[0m     \u001b[39m#time_folder = glob.glob(f'NewOpenFaceFeatures2/{name_video_folder}/*')\u001b[39;00m\n\u001b[1;32m      7\u001b[0m     time_folder \u001b[39m=\u001b[39m natsorted(os\u001b[39m.\u001b[39mlistdir(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mNewOpenFaceFeatures2/\u001b[39m\u001b[39m{\u001b[39;00mname_video_folder\u001b[39m}\u001b[39;00m\u001b[39m/\u001b[39m\u001b[39m'\u001b[39m))\n\u001b[1;32m      8\u001b[0m     \u001b[39m#print(natsorted(time_folder))\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'set_of_video' is not defined"
     ]
    }
   ],
   "source": [
    "column_names = [\"person0_features\", \"person1_features\", \"person2_features\",\n",
    "                \"person3_features\", \"id_video\", \"labels\"]\n",
    "\n",
    "result_csv = pd.DataFrame(columns = column_names)\n",
    "for name_video_folder in set_of_video:\n",
    "    #time_folder = glob.glob(f'NewOpenFaceFeatures2/{name_video_folder}/*')\n",
    "    time_folder = natsorted(os.listdir(f'NewOpenFaceFeatures2/{name_video_folder}/'))\n",
    "    #print(natsorted(time_folder))\n",
    "    for time_f_path in time_folder:\n",
    "        person_arr = []\n",
    "        id_video = \"\"\n",
    "        label = \"\"\n",
    "        temp2_csv = pd.DataFrame(columns = column_names)\n",
    "        for number_person in range(0,4):\n",
    "            temp_csv = pd.read_csv(f'NewOpenFaceFeatures2/{name_video_folder}/{time_f_path}/person{number_person}/person{number_person}.csv')\n",
    "            temp_csv.loc[temp_csv['confidence'] < 0.8, name_of_features] = 0\n",
    "            mean = np.mean(temp_csv[name_of_features].values.tolist(),axis=0)\n",
    "            std = np.std(temp_csv[name_of_features].values.tolist(),axis=0)\n",
    "            person_arr.append(np.concatenate([mean, std]))\n",
    "        temp2_csv[\"person0_features\"] = [person_arr[0]]\n",
    "        temp2_csv[\"person1_features\"] = [person_arr[1]]\n",
    "        temp2_csv[\"person2_features\"] = [person_arr[2]]\n",
    "        temp2_csv[\"person3_features\"] = [person_arr[3]]\n",
    "        temp2_csv[\"id_video\"] = name_video_folder\n",
    "        temp2_csv[\"labels\"] = person_num = re.findall(r'[0-9]$',time_f_path)[0]\n",
    "        result_csv = pd.concat([result_csv,temp2_csv])\n",
    "        \n",
    "        #print(result_csv[\"person0_features\"].values[0])\n",
    "            #person_arr.append(temp_csv[name_of_features].values.tolist())\n",
    "result_csv = result_csv.reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_csv.to_csv(\"OpenFaceFeatures.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_csv = pd.read_csv(\"OpenFaceFeatures.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "print(type(result_csv[\"person0_features\"].values[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create Dataset for pytorch\n",
    "class FeaturesDataset(torch.utils.data.Dataset):\n",
    "    \"\"\"Some Information about MyDataset\"\"\"\n",
    "    def __init__(self,csv_var):\n",
    "        super(FeaturesDataset, self).__init__()\n",
    "        self.csv_var = csv_var\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        person0 = self.csv_var['person0_features'][index].astype(np.float32)\n",
    "        person1 = self.csv_var['person1_features'][index].astype(np.float32)\n",
    "        person2 = self.csv_var['person2_features'][index].astype(np.float32)\n",
    "        person3 = self.csv_var['person3_features'][index].astype(np.float32)\n",
    "        label = int(self.csv_var[\"labels\"][index])\n",
    "        return person0, person1, person2, person3, label\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.csv_var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train, X_val, y_train, y_val = sklearn.model_selection.train_test_split(train_set.fname.values, train_set.label_encoded.values, \n",
    "                                                #   test_size=0.2, random_state=42)\n",
    "\n",
    "train_csv = result_csv[(result_csv['id_video'] != 'IS1003b') & (result_csv['id_video'] != 'IS1001c') & (result_csv['id_video'] != 'IS1008c')]\n",
    "train_csv = train_csv.reset_index(drop=True)\n",
    "val_csv = result_csv[(result_csv['id_video'] == 'IS1003b') | (result_csv['id_video'] == 'IS1001c')]\n",
    "val_csv = val_csv.reset_index(drop=True)\n",
    "test_csv = result_csv[(result_csv['id_video'] == 'IS1008c')]\n",
    "test_csv = test_csv.reset_index(drop=True)\n",
    "\n",
    "train_loader = DataLoader(FeaturesDataset(train_csv),batch_size=1, num_workers=8)\n",
    "val_loader = DataLoader(FeaturesDataset(val_csv),batch_size=1, num_workers=8)\n",
    "test_loader = DataLoader(FeaturesDataset(test_csv),batch_size=1, num_workers=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15\n"
     ]
    }
   ],
   "source": [
    "print(len(val_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]),\n",
       " tensor([[0.3926, 0.2126, 0.0577, 0.1203, 0.2548, 0.6150, 0.0751, 0.1703, 0.1835,\n",
       "          0.4268, 0.2937, 0.4099, 0.1523, 0.2212, 0.4920, 0.5147, 0.2141, 0.1667,\n",
       "          0.2333, 0.3133, 0.1200, 0.2267, 0.0067, 0.1467, 0.3400, 0.3100, 0.4933,\n",
       "          0.2667, 0.1867, 0.1133, 0.2467, 0.2700, 0.2533, 0.0700, 0.2100, 0.7018,\n",
       "          0.5484, 0.1180, 0.3207, 0.3774, 0.6417, 0.1775, 0.2770, 0.3416, 0.5497,\n",
       "          0.5317, 0.4553, 0.3313, 0.5260, 0.6184, 0.7112, 0.4080, 0.3727, 0.4230,\n",
       "          0.4638, 0.3250, 0.4187, 0.0814, 0.3538, 0.4737, 0.4625, 0.5000, 0.4422,\n",
       "          0.3896, 0.3170, 0.4311, 0.4440, 0.4349, 0.2551, 0.4073]]),\n",
       " tensor([[0.1852, 0.1529, 0.1733, 0.0611, 0.4118, 0.2637, 0.0538, 0.5065, 0.2015,\n",
       "          0.7906, 0.2079, 0.3240, 0.1234, 0.1152, 0.3188, 0.2627, 0.1076, 0.1100,\n",
       "          0.2133, 0.0567, 0.0267, 0.4233, 0.0967, 0.0000, 0.5800, 0.2700, 0.5033,\n",
       "          0.0667, 0.1767, 0.2467, 0.5867, 0.1133, 0.0767, 0.0000, 0.1267, 0.4443,\n",
       "          0.4561, 0.2850, 0.1906, 0.4134, 0.5312, 0.1318, 0.5601, 0.3263, 0.8162,\n",
       "          0.4095, 0.4207, 0.2786, 0.2468, 0.4110, 0.3958, 0.1930, 0.3129, 0.4097,\n",
       "          0.2312, 0.1611, 0.4941, 0.2955, 0.0000, 0.4936, 0.4440, 0.5000, 0.2494,\n",
       "          0.3814, 0.4311, 0.4924, 0.3170, 0.2661, 0.0000, 0.3326]]),\n",
       " tensor([[0.2469, 0.1358, 0.0563, 0.0772, 0.1473, 0.0883, 0.1081, 0.1250, 0.1357,\n",
       "          0.3384, 0.1681, 0.3321, 0.1044, 0.1518, 0.3833, 0.4118, 0.1344, 0.1467,\n",
       "          0.1233, 0.4333, 0.0400, 0.2767, 0.0133, 0.0367, 0.1300, 0.1933, 0.6300,\n",
       "          0.2100, 0.0000, 0.0700, 0.1367, 0.2167, 0.2067, 0.0767, 0.1167, 0.5617,\n",
       "          0.4026, 0.1500, 0.2195, 0.3271, 0.1861, 0.2413, 0.3355, 0.3093, 0.5498,\n",
       "          0.2975, 0.3590, 0.1990, 0.3084, 0.5288, 0.5340, 0.2221, 0.3538, 0.3288,\n",
       "          0.4955, 0.1960, 0.4474, 0.1147, 0.1879, 0.3363, 0.3949, 0.4828, 0.4073,\n",
       "          0.0000, 0.2551, 0.3435, 0.4120, 0.4049, 0.2661, 0.3210]]),\n",
       " tensor([1])]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "class Model(pl.LightningModule):\n",
    "    def __init__(self, in_features = 70, out_features = 4):\n",
    "        super(Model, self).__init__()\n",
    "        # self.l1   =  nn.Linear(in_features,in_features)\n",
    "        # self.l2   =  nn.Linear(in_features,in_features*2)\n",
    "        # self.l3   =  nn.Linear(in_features*2*4,in_features*2*4)\n",
    "        # self.l4   =  nn.Linear(in_features*2*4,out_features)\n",
    "        self.l1   =  nn.Linear(70,70)\n",
    "        self.l2   =  nn.Linear(70,128)\n",
    "        self.l3   =  nn.Linear(512,512)\n",
    "        self.l4   =  nn.Linear(512,4)\n",
    "        \n",
    "#         self.l1   =  nn.Linear(256,256)\n",
    "#         self.l2   =  nn.Linear(256,128)\n",
    "#         self.l3   =  nn.Linear(512,512)\n",
    "#         self.l4   =  nn.Linear(512,4)\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "        \n",
    "        #metrics\n",
    "        self.f1_train = torchmetrics.F1Score(num_classes=4)\n",
    "        self.f1_val = torchmetrics.F1Score(num_classes=4)\n",
    "        self.acc_train = torchmetrics.Accuracy(num_classes=4)\n",
    "        self.acc_val = torchmetrics.Accuracy(num_classes=4)\n",
    "        self.pr_train = torchmetrics.Precision(num_classes=4)\n",
    "        self.pr_val = torchmetrics.Precision(num_classes=4)\n",
    "        self.recall_train = torchmetrics.Recall(num_classes=4)\n",
    "        self.recall_val = torchmetrics.Recall(num_classes=4)\n",
    "        self.f1_test = torchmetrics.F1Score(num_classes=4)\n",
    "        self.acc_test = torchmetrics.Accuracy(num_classes=4)\n",
    "        self.pr_test = torchmetrics.Precision(num_classes=4)\n",
    "        self.recall_test = torchmetrics.Recall(num_classes=4)\n",
    "    \n",
    "        #loss\n",
    "        self.loss = torch.nn.CrossEntropyLoss()\n",
    "        \n",
    "    def forward(self, person1, person2, person3, person4):\n",
    "        p1 = self.dropout(self.l2(self.dropout(self.l1(person1))))\n",
    "        p2 = self.dropout(self.l2(self.dropout(self.l1(person2))))\n",
    "        p3 = self.dropout(self.l2(self.dropout(self.l1(person3))))\n",
    "        p4 = self.dropout(self.l2(self.dropout(self.l1(person4))))\n",
    "        com = torch.cat((p1,p2,p3,p4),dim=1)\n",
    "        com = torch.flatten(com)\n",
    "        x = self.dropout(self.l3(com))\n",
    "        x = self.dropout(self.l4(x))\n",
    "        return x #F.softmax(x) #Убрать     \n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "\n",
    "        p1,p2,p3,p4, y = batch\n",
    "        y_pred = self(p1,p2,p3,p4)\n",
    "        \n",
    "        loss = self.loss(y_pred,y[0])\n",
    "        \n",
    "        y_pred = y_pred.view(1,-1)\n",
    "  \n",
    "        \n",
    "        self.f1_train(y_pred, y)\n",
    "        self.acc_train(y_pred, y)\n",
    "        self.recall_train(y_pred, y)\n",
    "        self.pr_train(y_pred, y)\n",
    "        self.log(\"train_loss\", loss,prog_bar=True,on_epoch=True)\n",
    "        # self.log(\"train_acc\", self.acc_train, on_step=True, on_epoch=False, prog_bar=True)\n",
    "        # self.log(\"train_f1\", self.f1_train, on_step=True, on_epoch=False, prog_bar=True)\n",
    "        # self.f1_train(y_pred, y)\n",
    "        # self.acc_train(y_pred, y)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def training_epoch_end(self, outs):\n",
    "        f1 = self.f1_train.compute()\n",
    "        acc = self.acc_train.compute()\n",
    "        pr = self.pr_train.compute()\n",
    "        recall_var = self.recall_train.compute()\n",
    "        self.log('train_f1_epoch', f1)\n",
    "        self.log('train_acc_epoch', acc)\n",
    "        self.log('train_precision_epoch', pr)\n",
    "        self.log('train_recall_epoch', recall_var)\n",
    "        #self.log(\"train_acc\", self.train_acc.compute())\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        p1,p2,p3,p4, y = batch\n",
    "        y_pred = self(p1,p2,p3,p4)\n",
    "        #print(y)\n",
    "        #print(\"pred: \",y_pred,\"target: \",y)\n",
    "        \n",
    "        loss = self.loss(y_pred, y[0])\n",
    "        \n",
    "        y_pred = y_pred.view(1,-1)\n",
    "  \n",
    "        self.f1_val(y_pred, y)\n",
    "        self.acc_val(y_pred, y)\n",
    "        self.pr_val(y_pred, y)\n",
    "        self.recall_val(y_pred, y)\n",
    "        self.log(\"valid_loss\", loss, prog_bar=True)\n",
    "        #self.log(\"valid_acc\", self.acc_val, on_step=True, on_epoch=False, prog_bar=True)\n",
    "        #self.log(\"valid_f1\", self.f1_val, on_step=True, on_epoch=False, prog_bar=True)\n",
    "        #print(\"valid_loss\", loss)\n",
    "        #self.valid_acc.update(y_pred, y)\n",
    "        #self.log(\"valid_acc\", self.valid_acc.compute(), prog_bar=True)\n",
    "        # self.f1_val(y_pred, y)\n",
    "        # self.acc_val(y_pred, y)\n",
    "        return loss\n",
    "\n",
    "    def validation_epoch_end(self, outputs):\n",
    "        f1 = self.f1_val.compute()\n",
    "        acc = self.acc_val.compute()\n",
    "        pr = self.pr_val.compute()\n",
    "        recall_var = self.recall_val.compute()\n",
    "        self.log('val_f1_epoch', f1)\n",
    "        self.log('val_acc_epoch', acc)\n",
    "        self.log('val_precision_epoch', pr)\n",
    "        self.log('val_recall_epoch', recall_var)\n",
    "    \n",
    "    def test_step(self, batch, batch_idx):\n",
    "        p1,p2,p3,p4, y = batch\n",
    "        y_pred = self(p1,p2,p3,p4)\n",
    "        #print(y)\n",
    "        #print(\"pred: \",y_pred,\"target: \",y)\n",
    "        \n",
    "        loss = self.loss(y_pred, y[0])\n",
    "        \n",
    "        y_pred = y_pred.view(1,-1)\n",
    "  \n",
    "        self.f1_test(y_pred, y)\n",
    "        self.acc_test(y_pred, y)\n",
    "        self.pr_test(y_pred, y)\n",
    "        self.recall_test(y_pred, y)\n",
    "        self.log(\"test_loss\", loss, prog_bar=True)\n",
    "        #self.log(\"valid_acc\", self.acc_val, on_step=True, on_epoch=False, prog_bar=True)\n",
    "        #self.log(\"valid_f1\", self.f1_val, on_step=True, on_epoch=False, prog_bar=True)\n",
    "        #print(\"valid_loss\", loss)\n",
    "        #self.valid_acc.update(y_pred, y)\n",
    "        #self.log(\"valid_acc\", self.valid_acc.compute(), prog_bar=True)\n",
    "        # self.f1_val(y_pred, y)\n",
    "        # self.acc_val(y_pred, y)\n",
    "        return loss\n",
    "    \n",
    "    def test_epoch_end(self, outputs):\n",
    "        f1 = self.f1_test.compute()\n",
    "        acc = self.acc_test.compute()\n",
    "        pr = self.pr_test.compute()\n",
    "        recall_var = self.recall_test.compute()\n",
    "        self.log('test_f1_epoch', f1)\n",
    "        self.log('test_acc_epoch', acc)\n",
    "        self.log('test_precision_epoch', pr)\n",
    "        self.log('test_recall_epoch', recall_var)\n",
    "    \n",
    "    # def test_step(self, batch, batch_idx):\n",
    "    #     p1,p2,p3,p4, y = batch\n",
    "    #     #print(self(p1,p2,p3,p4))\n",
    "    #     y_pred = torch.flatten(self(p1,p2,p3,p4))\n",
    "        \n",
    "    #     loss = F.cross_entropy(y_pred.float(),y[0])\n",
    "    #     y_pred = torch.tensor([torch.argmax(y_pred)])\n",
    "    #     self.test_acc.update(y_pred, y) \n",
    "    #     self.log(\"test_loss\", loss, prog_bar=True) \n",
    "    #     self.log(\"test_acc\", self.test_acc.compute(), prog_bar=True) \n",
    "    #     return loss\n",
    "\n",
    "\n",
    "    # def test_epoch_end(self, outputs):\n",
    "    #     avg_loss = torch.stack([x[\"test_loss\"] for x in outputs]).mean()\n",
    "    #     logs = {\"test_loss\": avg_loss}\n",
    "    #     return {\"test_loss\": avg_loss, \"log\": logs, \"progress_bar\": logs}\n",
    "        \n",
    "    # def predict_step(self, batch, batch_idx, dataloader_idx=0):\n",
    "    #     x = batch\n",
    "    #     y_pred = self(x)\n",
    "    #     y_pred = torch.argmax(y_pred, dim=1)\n",
    "    #     y_pred = self.le.inverse_transform(y_pred.cpu().detach().numpy())\n",
    "    #     return y_pred\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.AdamW(self.parameters(), lr=1e-3)\n",
    "        return optimizer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CometLogger will be initialized in online mode\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "   | Name         | Type             | Params\n",
      "---------------------------------------------------\n",
      "0  | l1           | Linear           | 5.0 K \n",
      "1  | l2           | Linear           | 9.1 K \n",
      "2  | l3           | Linear           | 262 K \n",
      "3  | l4           | Linear           | 2.1 K \n",
      "4  | dropout      | Dropout          | 0     \n",
      "5  | f1_train     | F1Score          | 0     \n",
      "6  | f1_val       | F1Score          | 0     \n",
      "7  | acc_train    | Accuracy         | 0     \n",
      "8  | acc_val      | Accuracy         | 0     \n",
      "9  | pr_train     | Precision        | 0     \n",
      "10 | pr_val       | Precision        | 0     \n",
      "11 | recall_train | Recall           | 0     \n",
      "12 | recall_val   | Recall           | 0     \n",
      "13 | f1_test      | F1Score          | 0     \n",
      "14 | acc_test     | Accuracy         | 0     \n",
      "15 | pr_test      | Precision        | 0     \n",
      "16 | recall_test  | Recall           | 0     \n",
      "17 | loss         | CrossEntropyLoss | 0     \n",
      "---------------------------------------------------\n",
      "278 K     Trainable params\n",
      "0         Non-trainable params\n",
      "278 K     Total params\n",
      "1.115     Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "048268283b6f4c3fa23bf95feefe4c95",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "COMET WARNING: Comet has disabled auto-logging functionality as it has been imported after the following ML modules: torch. Metrics and hyperparameters can still be logged using comet_ml.log_metrics() and comet_ml.log_parameters()\n",
      "COMET WARNING: As you are running in a Jupyter environment, you will need to call `experiment.end()` when finished to ensure all metrics and code are logged before exiting.\n",
      "COMET INFO: Experiment is live on comet.com https://www.comet.com/philippshemetov/dominant/59451b0a6f2d4c759898a56aeac003ea\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=10` reached.\n",
      "COMET INFO: ---------------------------\n",
      "COMET INFO: Comet.ml Experiment Summary\n",
      "COMET INFO: ---------------------------\n",
      "COMET INFO:   Data:\n",
      "COMET INFO:     display_summary_level : 1\n",
      "COMET INFO:     url                   : https://www.comet.com/philippshemetov/dominant/59451b0a6f2d4c759898a56aeac003ea\n",
      "COMET INFO:   Metrics [count] (min, max):\n",
      "COMET INFO:     train_acc_epoch [10]       : (0.5572519302368164, 0.652671754360199)\n",
      "COMET INFO:     train_f1_epoch [10]        : (0.5572519302368164, 0.652671754360199)\n",
      "COMET INFO:     train_loss_epoch [10]      : (1.1819325685501099, 3.29459285736084)\n",
      "COMET INFO:     train_loss_step [26]       : (-0.0, 6.413330554962158)\n",
      "COMET INFO:     train_precision_epoch [10] : (0.5572519302368164, 0.652671754360199)\n",
      "COMET INFO:     train_recall_epoch [10]    : (0.5572519302368164, 0.652671754360199)\n",
      "COMET INFO:     val_acc_epoch [10]         : (0.0, 0.44736841320991516)\n",
      "COMET INFO:     val_f1_epoch [10]          : (0.0, 0.44736841320991516)\n",
      "COMET INFO:     val_precision_epoch [10]   : (0.0, 0.44736841320991516)\n",
      "COMET INFO:     val_recall_epoch [10]      : (0.0, 0.44736841320991516)\n",
      "COMET INFO:     valid_loss [10]            : (0.1048327311873436, 10.511529922485352)\n",
      "COMET INFO:   Others:\n",
      "COMET INFO:     Name : dominant8_LBPFeatures\n",
      "COMET INFO:   Uploads:\n",
      "COMET INFO:     environment details      : 1\n",
      "COMET INFO:     filename                 : 1\n",
      "COMET INFO:     git metadata             : 1\n",
      "COMET INFO:     git-patch (uncompressed) : 1 (331.16 KB)\n",
      "COMET INFO:     installed packages       : 1\n",
      "COMET INFO:     notebook                 : 1\n",
      "COMET INFO:     os packages              : 1\n",
      "COMET INFO:     source_code              : 1\n",
      "COMET INFO: ---------------------------\n",
      "COMET WARNING: Comet has disabled auto-logging functionality as it has been imported after the following ML modules: torch. Metrics and hyperparameters can still be logged using comet_ml.log_metrics() and comet_ml.log_parameters()\n",
      "COMET INFO: Uploading metrics, params, and assets to Comet before program termination (may take several seconds)\n",
      "COMET INFO: The Python SDK has 3600 seconds to finish before aborting...\n",
      "Restoring states from the checkpoint path at logs/dominant/59451b0a6f2d4c759898a56aeac003ea/checkpoints/epoch=9-step=1310.ckpt\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "Loaded model weights from checkpoint at logs/dominant/59451b0a6f2d4c759898a56aeac003ea/checkpoints/epoch=9-step=1310.ckpt\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d584ff48b81f4ea6b6c905206a5b07be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "COMET WARNING: Comet has disabled auto-logging functionality as it has been imported after the following ML modules: torch. Metrics and hyperparameters can still be logged using comet_ml.log_metrics() and comet_ml.log_parameters()\n",
      "COMET WARNING: As you are running in a Jupyter environment, you will need to call `experiment.end()` when finished to ensure all metrics and code are logged before exiting.\n",
      "COMET INFO: Experiment is live on comet.com https://www.comet.com/philippshemetov/dominant/59451b0a6f2d4c759898a56aeac003ea\n",
      "\n",
      "COMET INFO: -----------------------------------\n",
      "COMET INFO: Comet.ml ExistingExperiment Summary\n",
      "COMET INFO: -----------------------------------\n",
      "COMET INFO:   Data:\n",
      "COMET INFO:     display_summary_level : 1\n",
      "COMET INFO:     url                   : https://www.comet.com/philippshemetov/dominant/59451b0a6f2d4c759898a56aeac003ea\n",
      "COMET INFO:   Metrics:\n",
      "COMET INFO:     test_acc_epoch       : 0.20000000298023224\n",
      "COMET INFO:     test_f1_epoch        : 0.20000001788139343\n",
      "COMET INFO:     test_loss            : 10.845248222351074\n",
      "COMET INFO:     test_precision_epoch : 0.20000000298023224\n",
      "COMET INFO:     test_recall_epoch    : 0.20000000298023224\n",
      "COMET INFO:   Others:\n",
      "COMET INFO:     Name : dominant8_LBPFeatures\n",
      "COMET INFO: -----------------------------------\n",
      "COMET WARNING: Comet has disabled auto-logging functionality as it has been imported after the following ML modules: torch. Metrics and hyperparameters can still be logged using comet_ml.log_metrics() and comet_ml.log_parameters()\n",
      "COMET INFO: Uploading 2 metrics, params and output messages\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "       Test metric             DataLoader 0\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "     test_acc_epoch         0.20000000298023224\n",
      "      test_f1_epoch         0.20000001788139343\n",
      "        test_loss           10.845248222351074\n",
      "  test_precision_epoch      0.20000000298023224\n",
      "    test_recall_epoch       0.20000000298023224\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'test_loss': 10.845248222351074,\n",
       "  'test_f1_epoch': 0.20000001788139343,\n",
       "  'test_acc_epoch': 0.20000000298023224,\n",
       "  'test_precision_epoch': 0.20000000298023224,\n",
       "  'test_recall_epoch': 0.20000000298023224}]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pytorch_lightning.loggers import CometLogger\n",
    "\n",
    "model = Model(in_features=70)\n",
    "comet_logger = CometLogger(\n",
    "    api_key=\"hU25TlG9e0dRKvtdx1SwbIrHK\",\n",
    "    save_dir=\"logs\",\n",
    "    project_name=\"dominant\",\n",
    "    experiment_name=\"dominant8_LBPFeatures\",\n",
    "    # Optional\n",
    ")\n",
    "# experiment = comet_ml.Experiment(\n",
    "#     api_key=\"hU25TlG9e0dRKvtdx1SwbIrHK\",\n",
    "#     project_name=\"dominant\",\n",
    "# )\n",
    "trainer = pl.Trainer(max_epochs=10,accelerator=\"gpu\",devices=1,logger=comet_logger,gradient_clip_val=1)\n",
    "\n",
    "trainer.fit(model, train_dataloaders=train_loader,val_dataloaders=val_loader)\n",
    "\n",
    "#comet_logger.existing_experiment.log_asset(\"dominant8_LBPFeatures.ckpt\")\n",
    "\n",
    "trainer.test(dataloaders=test_loader)\n",
    "#trainer.test(val_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Restoring states from the checkpoint path at logs/dominant/59451b0a6f2d4c759898a56aeac003ea/checkpoints/epoch=9-step=1310.ckpt\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "Loaded model weights from checkpoint at logs/dominant/59451b0a6f2d4c759898a56aeac003ea/checkpoints/epoch=9-step=1310.ckpt\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c6c5057980b74653bc53506c8f8d1015",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "COMET WARNING: Comet has disabled auto-logging functionality as it has been imported after the following ML modules: torch. Metrics and hyperparameters can still be logged using comet_ml.log_metrics() and comet_ml.log_parameters()\n",
      "COMET WARNING: As you are running in a Jupyter environment, you will need to call `experiment.end()` when finished to ensure all metrics and code are logged before exiting.\n",
      "COMET INFO: Experiment is live on comet.com https://www.comet.com/philippshemetov/dominant/59451b0a6f2d4c759898a56aeac003ea\n",
      "\n",
      "COMET INFO: -----------------------------------\n",
      "COMET INFO: Comet.ml ExistingExperiment Summary\n",
      "COMET INFO: -----------------------------------\n",
      "COMET INFO:   Data:\n",
      "COMET INFO:     display_summary_level : 1\n",
      "COMET INFO:     url                   : https://www.comet.com/philippshemetov/dominant/59451b0a6f2d4c759898a56aeac003ea\n",
      "COMET INFO:   Metrics:\n",
      "COMET INFO:     test_acc_epoch       : 0.20000000298023224\n",
      "COMET INFO:     test_f1_epoch        : 0.20000001788139343\n",
      "COMET INFO:     test_loss            : 10.845248222351074\n",
      "COMET INFO:     test_precision_epoch : 0.20000000298023224\n",
      "COMET INFO:     test_recall_epoch    : 0.20000000298023224\n",
      "COMET INFO:   Others:\n",
      "COMET INFO:     Name : dominant8_LBPFeatures\n",
      "COMET INFO: -----------------------------------\n",
      "COMET WARNING: Comet has disabled auto-logging functionality as it has been imported after the following ML modules: torch. Metrics and hyperparameters can still be logged using comet_ml.log_metrics() and comet_ml.log_parameters()\n",
      "COMET INFO: Uploading 2 metrics, params and output messages\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "       Test metric             DataLoader 0\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "     test_acc_epoch         0.20000000298023224\n",
      "      test_f1_epoch         0.20000001788139343\n",
      "        test_loss           10.845248222351074\n",
      "  test_precision_epoch      0.20000000298023224\n",
      "    test_recall_epoch       0.20000000298023224\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'test_loss': 10.845248222351074,\n",
       "  'test_f1_epoch': 0.20000001788139343,\n",
       "  'test_acc_epoch': 0.20000000298023224,\n",
       "  'test_precision_epoch': 0.20000000298023224,\n",
       "  'test_recall_epoch': 0.20000000298023224}]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.test(dataloaders=test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ERROR: Could not find `tensorboard`. Please ensure that your PATH\n",
       "contains an executable `tensorboard` program, or explicitly specify\n",
       "the path to a TensorBoard binary by setting the `TENSORBOARD_BINARY`\n",
       "environment variable."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%load_ext tensorboard \n",
    "%tensorboard --logdir lightning_logs/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "next(iter(train_loader))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
